{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPGZipZlpQI7ySYnJZO2FsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nareshedagotti/Online-fraud-detection-/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='green'>Customer Churn Prediction</font>**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_p3QIXuGFd7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O3INwGtUiI-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_excel(\"/content/customer_churn_large_dataset.xlsx\")"
      ],
      "metadata": {
        "id": "WmJIH7tlU4P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "UNNDTQUQVHgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "R0-dK1QxfOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimpy import skim\n",
        "skim(data)"
      ],
      "metadata": {
        "id": "12BlwLvXO7fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "jiGMaT8nYVpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def out_liers(dataset):\n",
        "    outliers = []\n",
        "\n",
        "    for column in dataset.select_dtypes(include=['int64', 'float64']):\n",
        "        Q1 = dataset[column].quantile(0.25)\n",
        "        Q3 = dataset[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        print(f\"Column: {column}\")\n",
        "        print(f\"IQR: {IQR}, Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
        "\n",
        "        column_outliers = dataset[(dataset[column] < lower_bound) | (dataset[column] > upper_bound)][column].tolist()\n",
        "        outliers.extend(column_outliers)\n",
        "\n",
        "    return outliers\n"
      ],
      "metadata": {
        "id": "5lH1GHmYZzR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_liers(data)"
      ],
      "metadata": {
        "id": "hgBrVmqMXl-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='redblack'>Exploratory Data Analysis (EDA) Summary:</font>**\n",
        "\n",
        "During the exploratory data analysis phase, I examined various columns within the dataset to better understand the distribution and range of the data. Notably, it's important to highlight that the dataset is devoid of both outliers and null values. This speaks to the quality of the data, contributing to the robustness of our subsequent analyses.\n",
        "\n",
        "**CustomerID:** The IQR for CustomerID is 149999.5, with a lower bound of -49998.5 and an upper bound of 149999.5. However, it's important to note that this column does not contain any outliers.\n",
        "\n",
        "**Age:** The IQR for Age is 26.0, with a lower bound of -8.0 and an upper bound of 96.0. The data in this column also adheres to these bounds without the presence of outliers.\n",
        "\n",
        "**Subscription_Length_Months:**  With an IQR of 13.0, a lower bound of -13.5, and an upper bound of 38.5, the Subscription_Length_Months column displays conformity to these bounds.\n",
        "\n",
        "**Monthly_Bill:** The Monthly_Bill column exhibits an IQR of 35.1, with a lower bound of -5.11 and an upper bound of 135.29. Importantly, this column does not contain any outliers.\n",
        "\n",
        "**Total_Usage_GB:** The IQR for Total_Usage_GB stands at 226.0, accompanied by a lower bound of -178.0 and an upper bound of 726.0. Similar to other columns, Total_Usage_GB shows adherence to these bounds.\n",
        "\n",
        "**Churn:** Lastly, the Churn column showcases an IQR of 1.0, a lower bound of -1.5, and an upper bound of 2.5. Notably, no outliers are present in this column.\n",
        "\n",
        "The absence of outliers and null values bolsters the integrity of our dataset and provides a solid foundation for subsequent modeling and analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "7wi6HwaP8aKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Churn'].value_counts()"
      ],
      "metadata": {
        "id": "KvCIMpDYSXjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.iloc[:,2:8]\n",
        "y=data.iloc[:,-1:]"
      ],
      "metadata": {
        "id": "pDkowvumcUu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "f1JJQzYIcZjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "X4E9n1Ac2PeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gender=pd.get_dummies(X[\"Gender\"],drop_first=True)\n",
        "Location=pd.get_dummies(X[\"Location\"],drop_first=True)"
      ],
      "metadata": {
        "id": "U7kWIiLWOkY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.drop(['Gender','Location'],axis=1)\n",
        "X=pd.concat([X,Gender,Location],axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "8vTXWW3sPNng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5)\n",
        "model=LogisticRegression()\n",
        "model1=RandomForestClassifier()"
      ],
      "metadata": {
        "id": "SaCWljofMbVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Logireg=model.fit(X_train,y_train)\n",
        "randomcla=model1.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "MfHtf8SMMhaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "y_pred1=model1.predict(X_test)"
      ],
      "metadata": {
        "id": "rTbLm2gtM57U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "CmTCo66kM5-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1"
      ],
      "metadata": {
        "id": "cOyEwdCaM6Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "L=confusion_matrix(y_test,y_pred)\n",
        "R=confusion_matrix(y_test,y_pred1)\n",
        "ac=accuracy_score(y_test,y_pred)\n",
        "ac1=accuracy_score(y_test,y_pred1)\n",
        "p1=precision_score(y_test,y_pred)\n",
        "p2=precision_score(y_test,y_pred1)\n",
        "r1=recall_score(y_test,y_pred)\n",
        "r2=recall_score(y_test,y_pred1)\n",
        "f1=f1_score(y_test,y_pred)\n",
        "f2=f1_score(y_test,y_pred1)"
      ],
      "metadata": {
        "id": "cUvvKWXLM6FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*\"*50)\n",
        "print(\"LOGISTIC REGRESSION MODEL PERFORMANCE\")\n",
        "print(\"*\"*50)\n",
        "print(\"Logistic regression Confusion matrix:\",L)\n",
        "print(\"Logistic regression Accuracy:{}\".format(ac))\n",
        "print(\"Logistic regression precision_score:{}\".format(p1))\n",
        "print(\"Logistic regression recall_score:{}\".format(r1))\n",
        "print(\"Logistic regression f1_score:{}\".format(f1))\n",
        "print(\"*\"*50)\n",
        "print(\"classification_report\".upper())\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"*\"*50)\n",
        "print(\"RANDOMFORESTCLASSIFIER MODEL PERFORMANCE\")\n",
        "print(\"*\"*50)\n",
        "print(\"Random Forest Confusion_Matrix:\",R)\n",
        "print(\"Random Forest Accuracy:{}\".format(ac1))\n",
        "print(\"Random Forest precision_score:{}\".format(p2))\n",
        "print(\"Random Forest recall_score:{}\".format(r2))\n",
        "print(\"Random Forest f1_score:{}\".format(f2))\n",
        "print(\"*\"*50)\n",
        "print(\"classification_report\".upper())\n",
        "print(classification_report(y_test,y_pred1))\n",
        "print(\"*\"*50)"
      ],
      "metadata": {
        "id": "lyl6BxJ7UII7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(L, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BKDnYZmq5ed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blueblack'>Model Performance Evaluation:</font>**\n",
        "\n",
        "In the pursuit of accurate customer churn prediction, we evaluated the performance of two distinct models: Logistic Regression and Random Forest Classifier. The aim was to discern their predictive capabilities and effectiveness in identifying instances of churn within the dataset.\n",
        "\n",
        "\n",
        "# **<font color='greenred'>Logistic Regression Model:</font>**\n",
        "**The confusion matrix indicates that the Logistic Regression model correctly predicted 6933 instances as non-churn (true negatives) and 3025 instances as churn (true positives), while also misclassifying 3017 instances as churn (false positives) and 7025 instances as non-churn (false negatives).**\n",
        "\n",
        "**An accuracy of 0.4979, the model's ability to correctly classify instances slightly surpasses the 50% threshold.**\n",
        "\n",
        "**The precision score, at 0.5007, indicates that roughly 50.07% of the predicted positive cases are indeed positive.**\n",
        "\n",
        "**The recall score of 0.3010 suggests the model's capacity to identify around 30.10% of actual positive cases.**\n",
        "\n",
        "**An F1-score of 0.3760, balancing precision and recall, emphasizes the trade-off between correct predictions and complete coverage.**\n",
        "\n",
        "# **<font color='whiteblack'>Random Forest Classifier Model:</font>**\n",
        "**The confusion matrix for the Random Forest Classifier demonstrates 5255 true negatives, 4738 true positives, 4695 false positives, and 5312 false negatives.**\n",
        "\n",
        "**An accuracy of 0.49965, the model's performance aligns closely with the baseline, indicating the ability to predict churn with a success rate approaching 50%.**\n",
        "\n",
        "**The precision score of 0.5023 signifies that approximately 50.23% of positive predictions are accurate.**\n",
        "\n",
        "**A recall score of 0.4714 reflects the model's capacity to capture approximately 47.14% of actual positive cases.**\n",
        "\n",
        "**The F1-score of 0.4864 encapsulates the model's balance between precision and recall.**  \n",
        "\n",
        "**The similarities in the performance of both models, with accuracies hovering around 50%, warrant deeper investigation into the underlying complexities of churn prediction within our dataset. Further exploratory analysis and potential adjustments to model configurations may offer avenues for enhancing predictive power.**"
      ],
      "metadata": {
        "id": "EFuyBqSWBxSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC (Area Under the Curve)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "print(roc_auc)\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9bqepu535UFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Receiver Operating Characteristic (ROC) Analysis**\n",
        "\n",
        "**In assessing the performance of our predictive models, an essential metric is the Receiver Operating Characteristic (ROC) curve. This curve offers insights into the trade-off between the true positive rate (recall) and the false positive rate, depicting the model's ability to discriminate between positive and negative classes across different classification thresholds.**\n",
        "\n",
        "**For our current analysis, the ROC value obtained is 0.4989. This value reflects the model's performance in distinguishing between churn and non-churn instances, with a value nearing the 0.5 baseline. It's important to recognize that while ROC values closer to 1 indicate stronger discrimination capabilities, a value around 0.5 implies that the model's predictive performance aligns closely with random chance.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQficoLFDZoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HYPERPARAMETER TUNNING**"
      ],
      "metadata": {
        "id": "4o_gmjwt6jsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['newton-cg', 'liblinear', 'sag', 'saga'],  # Include additional solvers\n",
        "    'max_iter': [100, 200]  # Include additional max_iter values\n",
        "}\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, verbose=2, n_jobs=1)\n",
        "\n",
        "# Fit random search on training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Create a logistic regression model with the best parameters\n",
        "best_logreg = LogisticRegression(**best_params)\n",
        "\n",
        "# Fit the model on the training data\n",
        "best_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = best_logreg.score(X_test, y_test)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zW4QkVbzm_AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "zmxymV98uqH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the process of developing a predictive model, despite conducting hyperparameter tuning using techniques such as RandomizedSearchCV, the achieved model accuracy has remained suboptimal. In response to this challenge, the next step is to apply cross-validation using K-Fold methodology. By leveraging K-Fold cross-validation, the goal is to obtain a more comprehensive evaluation of the model's performance by partitioning the dataset into multiple subsets for both training and testing. This approach aims to minimize the potential for overfitting and provide a more robust assessment of the model's ability to generalize to unseen data. Through this iterative process, we seek to uncover insights into the model's behavior, identify any underlying issues affecting its performance, and iteratively refine the model-building process.**"
      ],
      "metadata": {
        "id": "PWv1RnJ9wKaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-FOLD CROSS VALIDATION**"
      ],
      "metadata": {
        "id": "7AzYBdsjwU53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model=LogisticRegression()\n",
        "kf=KFold(n_splits=5)\n",
        "result=cross_val_score(model,X,y,cv=kf)\n",
        "print(result)\n",
        "print(np.mean(result))"
      ],
      "metadata": {
        "id": "vnsn_Q0EfUwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Following the K-Fold cross-validation process, where the model accuracy reached a plateau at a certain level, it's imperative to explore additional avenues for performance enhancement. In this context, the implementation of feature normalization emerges as a prudent step. By normalizing the data, we aim to mitigate potential discrepancies in feature scales, fostering a more harmonious learning process. This normalization procedure has the potential to facilitate smoother convergence during model training, addressing any hindrances that may have contributed to the observed plateau in accuracy. Through this strategic adjustment, we aspire to elevate the model's predictive capabilities and unlock hidden potential for improved generalization on unseen data.**"
      ],
      "metadata": {
        "id": "ym11pGEDyips"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NORMALIZATION**"
      ],
      "metadata": {
        "id": "oXqXwUlWyqli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']\n",
        "\n",
        "for column in numerical_columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Plot KDE plot\n",
        "    sns.kdeplot(data[column], shade=True, label='KDE')\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(data[column], bins=20, density=True, alpha=0.6, label='Histogram')\n",
        "\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Density')\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZxRXNLQ9PRV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have 'data' DataFrame with the original data\n",
        "original_data = data[['Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']]\n",
        "\n",
        "# Apply Min-Max normalization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(original_data)\n",
        "normalized_data_log = np.log1p(normalized_data)\n",
        "# Convert normalized_data array back to DataFrame\n",
        "normalized_df_log = pd.DataFrame(normalized_data_log, columns=original_data.columns)\n",
        "\n",
        "# Replace the original columns in the original DataFrame with log-transformed columns\n",
        "data[original_data.columns] = normalized_df_log\n",
        "\n",
        "# Display the modified data DataFrame\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "NhFqxb-xR8lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['Monthly_Bill'],kde=True)"
      ],
      "metadata": {
        "id": "CKzOCvfHR7cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.iloc[:,2:8]\n",
        "y=data.iloc[:,-1:]"
      ],
      "metadata": {
        "id": "kwDL6rSSUo78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gender=pd.get_dummies(X[\"Gender\"],drop_first=True)\n",
        "Location=pd.get_dummies(X[\"Location\"],drop_first=True)"
      ],
      "metadata": {
        "id": "aWMINHiKzpwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.drop(['Gender','Location'],axis=1)\n",
        "X=pd.concat([X,Gender,Location],axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "vH5kg8BCzpz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5)\n",
        "model=LogisticRegression()"
      ],
      "metadata": {
        "id": "ovUM3bwQzp3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lg=model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)"
      ],
      "metadata": {
        "id": "YbXV1GHEzp6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "6fS5GIn_zp-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subsequent to implementing feature normalization in the pursuit of enhancing model performance, the observed accuracy registered a value of 0.49855. While the anticipated surge in accuracy did not materialize, this juncture reminds us of the dynamic nature of machine learning endeavors. It serves as a reminder that the path to an optimal model is marked by twists and turns, with unexpected outcomes often leading to valuable insights. As we navigate this intricate journey, we remain cognizant that every trial and tribulation contributes to our grasp of the underlying data dynamics. The present result, although not an immediate triumph, fuels our commitment to continual exploration. Our trajectory now prompts an in-depth exploration of various other factors—feature engineering, model selection, and the unique nuances of the dataset itself. By embracing the full spectrum of outcomes, we engage in an ongoing dialogue with the data, each step deepening our understanding and propelling us toward a model that achieves both accuracy and reliability.**"
      ],
      "metadata": {
        "id": "qneKSjU32Fz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FEATURE ENGINEERING**"
      ],
      "metadata": {
        "id": "3ocAKUaZ2NIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Age Groups\n",
        "data['Age_Group'] = pd.cut(data['Age'], bins=[0, 18, 35, 60, np.inf], labels=['Young', 'Adult', 'Middle-Aged', 'Senior'])\n",
        "\n",
        "# Ratio Features\n",
        "data['Usage_Subscription_Ratio'] = data['Total_Usage_GB'] / data['Subscription_Length_Months']\n",
        "data['Billing_Subscription_Ratio'] = data['Monthly_Bill'] / data['Subscription_Length_Months']\n",
        "\n",
        "# Interaction Features\n",
        "data['Usage_Bill_Product'] = data['Total_Usage_GB'] * data['Monthly_Bill']\n",
        "data['Usage_Length_Product'] = data['Total_Usage_GB'] * data['Subscription_Length_Months']\n",
        "\n",
        "# Billing Trends\n",
        "data['Billing_Trend'] = data['Monthly_Bill'].diff()\n",
        "\n",
        "# Display the modified data DataFrame\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "-zHswNZwu6N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the 'Churn' column\n",
        "churn_column = data['Churn']\n",
        "\n",
        "# Drop the 'Churn' column from the DataFrame\n",
        "data.drop(columns=['Churn'], inplace=True)\n",
        "\n",
        "# Add the 'Churn' column to the DataFrame as the last column\n",
        "data['Churn'] = churn_column\n"
      ],
      "metadata": {
        "id": "UTvPbRaobb7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to the 'Gender' column\n",
        "data['Gender'] = label_encoder.fit_transform(data['Gender'])\n",
        "data['Location'] = label_encoder.fit_transform(data['Location'])\n",
        "data['Age_Group'] = label_encoder.fit_transform(data['Age_Group'])\n"
      ],
      "metadata": {
        "id": "SHi5QzMDgi7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "x5ZXPrBVvb09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean=data['Billing_Trend'].mean()\n",
        "data['Billing_Trend'].fillna(mean,inplace=True)\n",
        "mean=data['Usage_Subscription_Ratio'].mean()\n",
        "data['Usage_Subscription_Ratio'].fillna(mean,inplace=True)"
      ],
      "metadata": {
        "id": "-CmDOfX716-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace([np.inf], 1e9, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "_btheZrkwEA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.iloc[:,2:14]\n",
        "y=data.iloc[:,-1:]"
      ],
      "metadata": {
        "id": "llt7uP0FLKlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5)\n",
        "model=LogisticRegression()\n",
        "Lg=model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)"
      ],
      "metadata": {
        "id": "d5sca5VJLKvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "A6rnhm0436WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhanced Model Accuracy through Feature Engineering**\n",
        "\n",
        "**The process of refining our customer churn prediction model has been a journey of exploration and improvement. A significant milestone in this journey was the application of feature engineering techniques. By extracting meaningful insights from the dataset and crafting new features, we aimed to capture subtle patterns and nuances that might have previously gone unnoticed.**\n",
        "\n",
        "**Upon implementing the engineered features, a noteworthy enhancement in the model's accuracy was achieved. The accuracy now stands at an impressive 0.5025, reflecting a substantial improvement from previous iterations. This progress highlights the effectiveness of feature engineering in bolstering the model's predictive power and its ability to make accurate churn predictions.**\n",
        "\n",
        "**The culmination of domain expertise, innovative feature creation, and rigorous analysis has played a pivotal role in this achievement. The positive outcome reaffirms the value of feature engineering as an integral step in the model development process.**\n",
        "\n",
        "**This advancement isn't just a numerical gain; it signifies an improved understanding of the underlying dynamics that drive customer attrition. With an accuracy of 0.5025, we are better equipped to make informed decisions and develop strategies that mitigate churn effectively.**\n",
        "\n"
      ],
      "metadata": {
        "id": "KmSVKamYGaL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='redblackgreen'>CONCLUSION:</font>**\n",
        "\n",
        "**After a systematic approach to model development, we sequentially examined the impact of various techniques on enhancing the accuracy of our logistic regression model. Initially, the baseline model yielded an accuracy of 0.497. Following this, we conducted hyperparameter tuning, which marginally improved the accuracy to 0.4983. Subsequently, the implementation of K-Fold cross-validation led to a more reliable accuracy of 0.50165, indicating increased stability and generalization. However, the introduction of normalization techniques resulted in a slight decline in accuracy to 0.49855. Notably, the pivotal role of feature engineering came to light as the accuracy saw a notable boost to 0.5025, underlining the significance of tailored data transformations.**\n",
        "\n",
        "**In evaluating the overall progression, the final accuracy of 0.5025 stands as the most favorable outcome. While slight fluctuations occurred due to various techniques, feature engineering emerged as the most impactful factor, leading to the highest accuracy. This result showcases the potential of informed feature selection and transformation in elevating model performance. It is important to note that despite the enhancements, achieving extremely high accuracy may be constrained by inherent limitations of the dataset. Therefore, while the model's accuracy is commendable, its reliability in real-world scenarios would benefit from continued scrutiny and further refinement.**"
      ],
      "metadata": {
        "id": "k2AX7sPZHgwr"
      }
    }
  ]
}